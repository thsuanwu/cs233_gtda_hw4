{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from collections import defaultdict\n",
    "\n",
    "## Imports based on our ready-to-use code (after you pip-install the cs233_gtda_hw4 package)\n",
    "from cs233_gtda_hw4.in_out.utils import make_data_loaders\n",
    "from cs233_gtda_hw4.in_out.utils import save_state_dicts, load_state_dicts\n",
    "from cs233_gtda_hw4.in_out import pointcloud_dataset\n",
    "from cs233_gtda_hw4.in_out.plotting import plot_3d_point_cloud\n",
    "\n",
    "\n",
    "## Imports you might use if you follow are scaffold code (it is OK to use your own stucture of the models)\n",
    "from cs233_gtda_hw4.models import PointcloudAutoencoder\n",
    "from cs233_gtda_hw4.models import PartAwarePointcloudAutoencoder\n",
    "from cs233_gtda_hw4.models.point_net import PointNet\n",
    "from cs233_gtda_hw4.models.mlp import MLP\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Fixed Settings (we do not expect you to change these)\n",
    "## \n",
    "\n",
    "n_points = 1024  # number of points of each point-cloud\n",
    "n_parts = 4      # max number of parts of each shape\n",
    "n_train_epochs = 400\n",
    "\n",
    "# Students: feel free to change below -ONLY- for the bonus Question:\n",
    "# I.e., use THESE hyper-parameters when you train for the non-bonus questions.\n",
    "\n",
    "part_lambda = 0.005  # for the part-aware AE you will be using (summing) two losses:\n",
    "                     # chamfer + cross-entropy\n",
    "                     # do it like this: chamfer + (part_lambda * cross-entropy), \n",
    "                     # i.e. we are scaling down the cross-entropy term\n",
    "init_lr = 0.009  # initial learning-rate, tested by us with ADAM optimizer (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Students: feel free to change below:\n",
    "\n",
    "# batch-size of data loaders\n",
    "batch_size = 128 # if you can keep this too as is keep it, \n",
    "                 # but if it is too big for your GPU, feel free to change it.\n",
    "\n",
    "# which device to use: cpu or cuda?\n",
    "device = 'cpu'     # Note: only the \"alternative\" (slower) chamfer_loss in losses/nn_distance can run in cpu.\n",
    "#device = 'cuda'\n",
    "\n",
    "top_in_dir = '../data/'\n",
    "top_out_dir = '../data/out/'\n",
    "if not osp.exists(top_out_dir):\n",
    "    os.makedirs(top_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-examples train 750\n",
      "N-examples test 150\n",
      "N-examples val 50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m LATENT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# batch_size, num_channels, num_points\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPointNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_CHANNELS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m decoder \u001b[38;5;241m=\u001b[39m MLP(LATENT_DIM, NUM_POINTS)\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/cs233_gtda_hw4/models/point_net.py:25\u001b[0m, in \u001b[0;36mPointNet.__init__\u001b[0;34m(self, init_feat_dim, conv_dims, student_optional_hyper_param)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, init_feat_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, conv_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m], student_optional_hyper_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Students:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    You can make a generic function that instantiates a point-net with arbitrary hyper-parameters,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    :param conv_dims: output point dimensionality of each layer\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m \u001b[38;5;66;03m#batch_size, num_channels, num_points\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPointNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     27\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv1d(init_feat_dim, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     28\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool1d(\u001b[38;5;241m1024\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA:\n",
    "\n",
    "loaders = make_data_loaders(top_in_dir, batch_size)\n",
    "\n",
    "for split, loader in loaders.items():\n",
    "    print('N-examples', split, len(loader.dataset))\n",
    "    \n",
    "# BUILD MODELS:\n",
    "### TODO: Student on your own:\n",
    "NUM_POINTS = 1024\n",
    "NUM_CHANNELS = 3\n",
    "INPUT_DIM = (batch_size, NUM_CHANNELS, NUM_POINTS)\n",
    "LATENT_DIM = 128\n",
    "# batch_size, num_channels, num_points\n",
    "\n",
    "encoder = PointNet(NUM_CHANNELS)\n",
    "decoder = MLP(LATENT_DIM, NUM_POINTS)\n",
    "#part_classifier = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     model_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpart_pc_ae\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mPointcloudAutoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Students Work here\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     model_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpc_ae\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/cs233_gtda_hw4/models/pointcloud_autoencoder.py:25\u001b[0m, in \u001b[0;36mPointcloudAutoencoder.__init__\u001b[0;34m(self, encoder, decoder)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder, decoder):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124;03m\"\"\" AE constructor.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    :param encoder: nn.Module acting as a point-cloud encoder.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    :param decoder: nn.Module acting as a point-cloud decoder.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPointcloudAutoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m encoder\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m decoder\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "part_aware_model = False # or True\n",
    "\n",
    "if part_aware_model:\n",
    "    xentropy = nn.CrossEntropyLoss()\n",
    "    model = PartAwarePointcloudAutoencoder().to(device) # Students Work here\n",
    "    model_tag = 'part_pc_ae'\n",
    "else:\n",
    "    model = PointcloudAutoencoder(encoder, decoder).to(device)  # Students Work here\n",
    "    model_tag = 'pc_ae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=init_lr)  # Students uncomment once you have defined your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                            | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1024, 3])\n",
      "embed torch.Size([128, 128, 1])\n",
      "squeezed torch.Size([128, 128])\n",
      "decoder torch.Size([128, 1024])\n",
      "torch.Size([128, 1024, 3])\n",
      "embed torch.Size([128, 128, 1])\n",
      "squeezed torch.Size([128, 128])\n",
      "decoder torch.Size([128, 1024])\n",
      "torch.Size([128, 1024, 3])\n",
      "embed torch.Size([128, 128, 1])\n",
      "squeezed torch.Size([128, 128])\n",
      "decoder torch.Size([128, 1024])\n",
      "torch.Size([128, 1024, 3])\n",
      "embed torch.Size([128, 128, 1])\n",
      "squeezed torch.Size([128, 128])\n",
      "decoder torch.Size([128, 1024])\n",
      "torch.Size([128, 1024, 3])\n",
      "embed torch.Size([128, 128, 1])\n",
      "squeezed torch.Size([128, 128])\n",
      "decoder torch.Size([128, 1024])\n",
      "torch.Size([110, 1024, 3])\n",
      "embed torch.Size([110, 128, 1])\n",
      "squeezed torch.Size([110, 128])\n",
      "decoder torch.Size([110, 1024])\n",
      "torch.Size([50, 1024, 3])\n",
      "embed torch.Size([50, 128, 1])\n",
      "squeezed torch.Size([50, 128])\n",
      "decoder torch.Size([50, 1024])\n",
      "torch.Size([128, 1024, 3])\n",
      "embed torch.Size([128, 128, 1])\n",
      "squeezed torch.Size([128, 128])\n",
      "decoder torch.Size([128, 1024])\n",
      "torch.Size([22, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                   | 1/400 [00:03<22:11,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed torch.Size([22, 128, 1])\n",
      "squeezed torch.Size([22, 128])\n",
      "decoder torch.Size([22, 1024])\n",
      "torch.Size([128, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Students Work Here.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_for_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/cs233_gtda_hw4/models/pointcloud_autoencoder.py:53\u001b[0m, in \u001b[0;36mPointcloudAutoencoder.train_for_one_epoch\u001b[0;34m(self, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Check dims\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(point_cloud\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m chamfer_loss(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint_cloud\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, point_cloud)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     54\u001b[0m     loss_meter\u001b[38;5;241m.\u001b[39mupdate(loss, point_cloud\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_meter\u001b[38;5;241m.\u001b[39mavg\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/cs233_gtda_hw4/models/pointcloud_autoencoder.py:74\u001b[0m, in \u001b[0;36mPointcloudAutoencoder.reconstruct\u001b[0;34m(self, loader, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreconstruct\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124;03m\"\"\" Reconstruct the point-clouds via the AE.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :param loader: pointcloud_dataset loader\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    :param device: cpu? cuda?\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    :return: Left for students to decide\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     76\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(x)\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/cs233_gtda_hw4/models/pointcloud_autoencoder.py:64\u001b[0m, in \u001b[0;36mPointcloudAutoencoder.embed\u001b[0;34m(self, pointclouds)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, pointclouds):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124;03m\"\"\" Extract from the input pointclouds the corresponding latent codes.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    :param pointclouds: B x N x 3\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    :return: B x latent-dimension of AE\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpointclouds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/cs233_gtda_hw4/models/point_net.py:47\u001b[0m, in \u001b[0;36mPointNet.forward\u001b[0;34m(self, pointclouds)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mRun forward pass of the PointNet model on a given point cloud.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m:param pointclouds: (B x N x 3) point cloud\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m pointclouds\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# transpose since nn.Conv1d expects the inputs to be dimension (batch_size, num_channels, num_points).\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#print('Pointnet, before pooling:',x.shape)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repository/cs233_gtda_hw4/hw4_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    296\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    297\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Train for multiple epochs your model.\n",
    "# Students: the below for-loops are optional, feel free to structure your training \n",
    "# differently.\n",
    "\n",
    "min_val_loss = np.Inf\n",
    "out_file = osp.join(top_out_dir, model_tag + 'best_model.pth')\n",
    "start_epoch = 1\n",
    "\n",
    "for epoch in tqdm.tqdm(range(start_epoch, start_epoch + n_train_epochs)):\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        pass\n",
    "        \n",
    "        # Students Work Here.\n",
    "        epoch_losses = model.train_for_one_epoch(loaders[phase], optimizer, device)\n",
    "\n",
    "##       Save model if validation loss improved.\n",
    "#         if phase == 'val' and recon_loss < min_val_loss:\n",
    "#             min_val_loss = recon_loss\n",
    "            \n",
    "##        If you save the model like this, you can use the code below to load it. \n",
    "#             save_state_dicts(out_file, epoch=epoch, model=model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with best per-validation loss (uncomment when ready)\n",
    "# best_epoch = load_state_dicts(out_file, model=model)\n",
    "# print('per-validation optimal epoch', best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students TODO: MAKE your plots and analysis\n",
    "\n",
    "# 5 examples to visualize per questions (e, f)\n",
    "examples_to_visualize = ['8a67fd47001e52414c350d7ea5fe2a3a',\n",
    "                         '1e0580f443a9e6d2593ebeeedbff73b',\n",
    "                         'd3562f992aa405b214b1fd95dbca05',\n",
    "                         '4e8d8792a3a6390b36b0f2a1430e993a',\n",
    "                         '58479a7b7c157865e68f66efebc71317']\n",
    "\n",
    "# You can (also) use the function for the reconstructions or the part-predictions \n",
    "# (for the latter check the kwargs parameter 'c' of matplotlib.\n",
    "    # plot_3d_point_cloud, eg. try plot_3d_point_cloud(loaders['test'].dataset.pointclouds[0])\n",
    "    \n",
    "model.eval()   # Do not forget this.! We are not training any more (OK, since we do not \n",
    "               # have batch-norm, drop-out etc. this is not so important, however it is good standard \n",
    "               # practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last, save the latent codes of the test data and go to the \n",
    "# measuring_part_awareness and tsne_plot_with_latent_codes code.\n",
    "\n",
    "latent_codes = []\n",
    "\n",
    "# Students TODO: Extract the latent codes and save them, so you can analyze them later.\n",
    "\n",
    "\n",
    "np.savez(osp.join(top_out_dir, model_tag +'_latent_codes'), \n",
    "         latent_codes=latent_codes, \n",
    "         test_names=test_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
