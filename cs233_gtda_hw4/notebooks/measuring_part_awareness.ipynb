{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import NearestNeighbors  # Students: you can use this implementation to find the \n",
    "                                                # Nearest-Neigbors\n",
    "from cs233_gtda_hw4.in_out.plotting import plot_3d_point_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA Tesla K80'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from collections import defaultdict\n",
    "\n",
    "## Imports based on our ready-to-use code (after you pip-install the cs233_gtda_hw4 package)\n",
    "from cs233_gtda_hw4.in_out.utils import make_data_loaders\n",
    "from cs233_gtda_hw4.in_out.utils import save_state_dicts, load_state_dicts\n",
    "from cs233_gtda_hw4.in_out import pointcloud_dataset\n",
    "from cs233_gtda_hw4.in_out.plotting import plot_3d_point_cloud\n",
    "\n",
    "\n",
    "## Imports you might use if you follow are scaffold code (it is OK to use your own stucture of the models)\n",
    "from cs233_gtda_hw4.models import PointcloudAutoencoder\n",
    "from cs233_gtda_hw4.models import PartAwarePointcloudAutoencoder\n",
    "from cs233_gtda_hw4.models.point_net import PointNet\n",
    "from cs233_gtda_hw4.models.mlp import MLP\n",
    "from cs233_gtda_hw4.models.part_classifier import part_classifier\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Fixed Settings (we do not expect you to change these)\n",
    "## \n",
    "\n",
    "n_points = 1024  # number of points of each point-cloud\n",
    "n_parts = 4      # max number of parts of each shape\n",
    "n_train_epochs = 400\n",
    "\n",
    "# Students: feel free to change below -ONLY- for the bonus Question:\n",
    "# I.e., use THESE hyper-parameters when you train for the non-bonus questions.\n",
    "\n",
    "part_lambda = 0.005  # for the part-aware AE you will be using (summing) two losses:\n",
    "                     # chamfer + cross-entropy\n",
    "                     # do it like this: chamfer + (part_lambda * cross-entropy), \n",
    "                     # i.e. we are scaling down the cross-entropy term\n",
    "init_lr = 0.009  # initial learning-rate, tested by us with ADAM optimizer (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Students: feel free to change below:\n",
    "\n",
    "# batch-size of data loaders\n",
    "batch_size = 128 # if you can keep this too as is keep it, \n",
    "                 # but if it is too big for your GPU, feel free to change it.\n",
    "\n",
    "# which device to use: cpu or cuda?\n",
    "#device = 'cpu'     # Note: only the \"alternative\" (slower) chamfer_loss in losses/nn_distance can run in cpu.\n",
    "device = 'cuda'\n",
    "\n",
    "top_in_dir = '../data/'\n",
    "top_out_dir = '../data/out/'\n",
    "if not osp.exists(top_out_dir):\n",
    "    os.makedirs(top_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-examples train 750\n",
      "N-examples test 150\n",
      "N-examples val 50\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA:\n",
    "\n",
    "loaders = make_data_loaders(top_in_dir, batch_size)\n",
    "\n",
    "for split, loader in loaders.items():\n",
    "    print('N-examples', split, len(loader.dataset))\n",
    "    \n",
    "# BUILD MODELS:\n",
    "### TODO: Student on your own:\n",
    "NUM_POINTS = 1024\n",
    "NUM_CHANNELS = 3\n",
    "LATENT_DIM = 128\n",
    "# batch_size, num_channels, num_points\n",
    "\n",
    "encoder = PointNet(NUM_CHANNELS)\n",
    "decoder = MLP(LATENT_DIM, NUM_POINTS)\n",
    "part_classifier = part_classifier(LATENT_DIM+NUM_CHANNELS, n_parts, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students: Default location of saved latent codes per last cell of main.ipynb, change appropriately if\n",
    "# you saved them in another way.\n",
    "vanilla_ae_emb_file = '../data/out/pc_ae_latent_codes.npz'\n",
    "part_ae_emb_file = '../data/out/part_pc_ae_latent_codes.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n",
      "c59cdd1537bd75ddd0818327fc390a5__2__\n"
     ]
    }
   ],
   "source": [
    "# Load golden distances (pairwise matrix, or corresponding model/part names in golden_names)\n",
    "golden_part_dist_file = '../data/golden_dists.npz'\n",
    "golden_data = np.load(golden_part_dist_file, allow_pickle=True)\n",
    "golden_part_dist = golden_data['golden_part_dist']\n",
    "golden_names = golden_data['golden_names']\n",
    "print(len(golden_names))  # models-name/part combinations\n",
    "print(golden_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compare distances with vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load vanilla-AE-embeddings (if False will open those of the 2-branch AE).\n",
    "vanilla = True # or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/organize golden part-aware distances.\n",
    "sn_id_to_parts = defaultdict(list)\n",
    "id_to_part_loc = dict()\n",
    "\n",
    "for i, name in enumerate(golden_names):\n",
    "    # Extract shape-net model ids of golden, map them to their parts.\n",
    "    sn_id, _, part_id, _, _ = name.split('_')\n",
    "    sn_id_to_parts[sn_id].append(part_id)\n",
    "    \n",
    "    # Map shape-net model id and part_id to location in distance matrix, (the order is the same).\n",
    "    id_to_part_loc[(sn_id, part_id)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vanilla:\n",
    "    in_d = np.load(vanilla_ae_emb_file)    # Students: assuming you used the numpy.savez\n",
    "else:\n",
    "    in_d = np.load(part_ae_emb_file)\n",
    "        \n",
    "latent_codes = in_d['latent_codes']\n",
    "test_names = in_d['test_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the provided part-distance function dP, compare the cumulative distances of the encoding space learned by the vanilla AE of (d) vs. the part-aware AE\n",
    "of (f). Compute the cumulative distance of an encoding space by accumulating the part\n",
    "distances of the parts of every chair in the test split, to those of its nearest neighbor (NN)\n",
    "in the encoding. Use the Euclidean distance between the latent vectors to compute the\n",
    "neighborhoods. Let M(A) denote all parts of chair A and MËœ(A, k) its k-th part. Define the\n",
    "one-way (part-based) distance of chair A from B as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=2)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "neigh.fit(latent_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use golden distances and matchings to solve question (g)\n",
    "\n",
    "distances = []\n",
    "num_parts_shared_all = []\n",
    "latent_distances = []\n",
    "for i, sn_name in enumerate(test_names):\n",
    "    parts_of_model = set(sn_id_to_parts[sn_name])\n",
    "    \n",
    "    # Find nearest neighbor to model sn_name\n",
    "    nn_i = neigh.kneighbors(latent_codes[i,].reshape(1, -1), return_distance=True)[1][:,1].item()\n",
    "    nn_dist = neigh.kneighbors(latent_codes[i,].reshape(1, -1), return_distance=True)[0][:,1].item()\n",
    "    latent_distances.append(nn_dist)\n",
    "    matched_neighbor = test_names[nn_i]\n",
    "    parts_of_neighbor = set(sn_id_to_parts[matched_neighbor])\n",
    "    \n",
    "    # Compute the requested distances.\n",
    "    # Use id_to_part_loc for each model/part combination\n",
    "    \n",
    "    distance = 0 # initiate per model distance\n",
    "    num_parts_shared = 0\n",
    "    \n",
    "    for k in parts_of_model:\n",
    "        if k in parts_of_neighbor:\n",
    "            num_parts_shared += 1\n",
    "            a_id = id_to_part_loc[(sn_name, k)]\n",
    "            b_id = id_to_part_loc[(matched_neighbor, k)]\n",
    "            distance += golden_part_dist[a_id, b_id]\n",
    "        else:\n",
    "            temp_dists = []\n",
    "            for u in parts_of_neighbor:\n",
    "                a_id = id_to_part_loc[(sn_name, k)]\n",
    "                b_id = id_to_part_loc[(matched_neighbor, u)]\n",
    "                temp_dists.append(golden_part_dist[a_id, b_id])\n",
    "            distance += max(temp_dists)\n",
    "    distances.append(distance)\n",
    "    num_parts_shared_all.append(num_parts_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative distance:  401.75394278764725\n",
      "average number of shared parts:  3.1533333333333333\n",
      "average euclidean distance between nearest neighbors:  0.21923460632562639\n"
     ]
    }
   ],
   "source": [
    "print(\"cumulative distance: \",sum(distances))\n",
    "print(\"average number of shared parts: \", sum(num_parts_shared_all) / len(num_parts_shared_all))\n",
    "print(\"average euclidean distance between nearest neighbors: \", sum(latent_distances) / len(latent_distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compare distances with part-aware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load vanilla-AE-embeddings (if False will open those of the 2-branch AE).\n",
    "vanilla = False # or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/organize golden part-aware distances.\n",
    "sn_id_to_parts = defaultdict(list)\n",
    "id_to_part_loc = dict()\n",
    "\n",
    "for i, name in enumerate(golden_names):\n",
    "    # Extract shape-net model ids of golden, map them to their parts.\n",
    "    sn_id, _, part_id, _, _ = name.split('_')\n",
    "    sn_id_to_parts[sn_id].append(part_id)\n",
    "    \n",
    "    # Map shape-net model id and part_id to location in distance matrix, (the order is the same).\n",
    "    id_to_part_loc[(sn_id, part_id)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vanilla:\n",
    "    in_d = np.load(vanilla_ae_emb_file)    # Students: assuming you used the numpy.savez\n",
    "else:\n",
    "    in_d = np.load(part_ae_emb_file)\n",
    "        \n",
    "latent_codes = in_d['latent_codes']\n",
    "test_names = in_d['test_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the provided part-distance function dP, compare the cumulative distances of the encoding space learned by the vanilla AE of (d) vs. the part-aware AE\n",
    "of (f). Compute the cumulative distance of an encoding space by accumulating the part\n",
    "distances of the parts of every chair in the test split, to those of its nearest neighbor (NN)\n",
    "in the encoding. Use the Euclidean distance between the latent vectors to compute the\n",
    "neighborhoods. Let M(A) denote all parts of chair A and MËœ(A, k) its k-th part. Define the\n",
    "one-way (part-based) distance of chair A from B as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=2)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "neigh.fit(latent_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3.7252903e-09, 1.6370015e-01]], dtype=float32), array([[100, 120]]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.kneighbors(latent_codes[100,].reshape(1, -1), return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use golden distances and matchings to solve question (g)\n",
    "\n",
    "distances = []\n",
    "num_parts_shared_all = []\n",
    "latent_distances = []\n",
    "for i, sn_name in enumerate(test_names):\n",
    "    parts_of_model = set(sn_id_to_parts[sn_name])\n",
    "    #print(i, \",\", sn_name)\n",
    "    #print(parts_of_model)\n",
    "    #plot_3d_point_cloud(loaders['test'].dataset.pointclouds[i], title = 'original',c = loaders['test'].dataset.part_masks[i])\n",
    "    \n",
    "    # Find nearest neighbor to model sn_name\n",
    "    nn_i = neigh.kneighbors(latent_codes[i,].reshape(1, -1), return_distance=True)[1][:,1].item()\n",
    "    nn_dist = neigh.kneighbors(latent_codes[i,].reshape(1, -1), return_distance=True)[0][:,1].item()\n",
    "    latent_distances.append(nn_dist)\n",
    "    matched_neighbor = test_names[nn_i]\n",
    "    parts_of_neighbor = set(sn_id_to_parts[matched_neighbor])\n",
    "    \n",
    "    #print(nn_i, \",\", matched_neighbor, \"distance: \", nn_dist)\n",
    "    #print(parts_of_neighbor)\n",
    "    #plot_3d_point_cloud(loaders['test'].dataset.pointclouds[nn_i], title = 'original',c = loaders['test'].dataset.part_masks[nn_i])\n",
    "    \n",
    "    # Compute the requested distances.\n",
    "    # Use id_to_part_loc for each model/part combination\n",
    "    distance = 0 # initiate per model distance\n",
    "    num_parts_shared = 0\n",
    "    \n",
    "    for k in parts_of_model:\n",
    "        if k in parts_of_neighbor:\n",
    "            num_parts_shared += 1\n",
    "            a_id = id_to_part_loc[(sn_name, k)]\n",
    "            b_id = id_to_part_loc[(matched_neighbor, k)]\n",
    "            distance += golden_part_dist[a_id, b_id]\n",
    "        else:\n",
    "            #print(k)\n",
    "            temp_dists = []\n",
    "            for u in parts_of_neighbor:\n",
    "                a_id = id_to_part_loc[(sn_name, k)]\n",
    "                b_id = id_to_part_loc[(matched_neighbor, u)]\n",
    "                #print(golden_part_dist[a_id, b_id])\n",
    "                temp_dists.append(golden_part_dist[a_id, b_id])\n",
    "            distance += max(temp_dists)\n",
    "    distances.append(distance)\n",
    "    num_parts_shared_all.append(num_parts_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative distance:  396.53958693146706\n",
      "average number of shared parts:  3.18\n",
      "average euclidean distance between nearest neighbors:  0.26441301822662355\n"
     ]
    }
   ],
   "source": [
    "print(\"cumulative distance: \",sum(distances))\n",
    "print(\"average number of shared parts: \", sum(num_parts_shared_all) / len(num_parts_shared_all))\n",
    "print(\"average euclidean distance between nearest neighbors: \", sum(latent_distances) / len(latent_distances))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
